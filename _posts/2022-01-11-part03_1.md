---
layout: single
title: "D&A study part03_Dropout~Optimizer"
---

# 01. Dropout
- 신경망의 학습과정 중 Layer 노드를 랜덤하게 Drop함으로써 Generalization 효과를 가져오게 하는 테크닉이다.
- Weight Matrix에 랜덤하게 일부 Column에 0을 집어넣어 연산을 하는 것으로 이해할 수 있다.
- Dropout을 적용할 때에는 얼마나 랜덤하게 Dropout 기법을 적용할 것인지에 대한 확률값을 지정해야 하고, 이는 Input layer와 Hidden layer에 적용할 수 있다
- Epoch마다 랜덤하게 Droupout을 한다. 즉, 이전  Epoch와 다른 column에 0을 집어넣는 것이라 이해할 수 있다.
- Droupout은 학습 시에만 적용이 되기 때문에 테스트를 할 때에는 모든 노드를 Turn-on 해준 상태로 진행된다.
- 계산 비용이 작아 학습이 빠르고, 대부분의 인공신경망 모델에서 사용가능하다는 장점이 있다. 또, 매번 랜덤하게 Droupout이 되기 때문에 여러 인공신경망 모델을 앙상블 하는 것과 비슷하게 일반화 성능을 향상시켜 (= 각 Sub 모델들에 의한 예측이 서로 다른 오차를 갖기 때문에 일반화 성능을 향상시켜) 과대적합을 방지한다는 장점이 있다.

![dropout.PNG](attachment:dropout.PNG)
<center> <font color=grey> [출처]https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf  </font> </center>

# 02. Activation 함수
### ReLU 함수
- 비선형 활성함수가 가지고 있는 문제점을 어느정도 해결한 활성함수이다.
- $f(x) = max(0,x)$와 같이 정의되며, 입력 값이 0 이상이면 그 값을 그대로 출력하고, 0 이하이면 0으로 출력하는 함수로 미분을 할 때에 0 이상인 부분은 1, 0 이하인 부분은 0이 된다. 즉, Back Propagation 과정 중 곱해지는 Activation 미분 값이 0 또는 1이 되어 아예 없애거나 완전히 살리는 것이다.
- 따라서 Hidden Layer가 깊어져도 Gradient Vanishing이 일어나는 것을 완화시킴으로 Layer를 깊게 쌓아 복잡한 모형을 만들 수 있게 되었다.
- 이 밖에도 Leaky ReLU, ELU, paramertric ReLU, SELU, SERLU 등 ReLU 함수의 변형함수가 많이 등장하였다.

# 03. Batch Normalization
※ Internal Covariance Shift : 각 Layer마다 Input의 분포가 달라짐에 따라 학습 속도가 느려지는 현상
- Bach Normalization은 Internal Covariance Shift를 방지하기 위한 기법으로, Layer의 Input 분포를 정규화하여 학습속도를 빠르게 하는 것이다.
- $BN(h;\gamma,\beta) = \beta+\gamma \frac{h-E(h}{\sqrt{(Var(h)+\epsilon)}}$
    - $h$는 Input의 분포 ($Beta(\beta)$와 $Gamma(\gamma)$가 없다고 가정하면 정규화하는 수식과 일치)
    - $Beta(\beta)$와 $Gamma(\gamma)$는 분포를 shift시키고 Scaling 시키는 값으로 Back Propagation을 통해 학습시킨다.
- Bach Normalization은 학습 속도를 향상시켜주고, Gradient Vanishing 문제를 완화해주는 장점이 있다.

![image.png](attachment:image.png)
  <center> <font color=grey> [출처]https://guillaumebrg.wordpress.com/2016/02/28/dogs-vs-cats-adding-batch-normalization-5-1-error-rate/   </font> </center>  
  
- Batch Normalization을 통해 정규분포와 비슷한 형태로 정규화되고 있다.
- 만약, Batch Normalization을 하지 않았다면 ReLU 함수는 0 이하는 0, 0 이상은 그 값을 출력해주는 함수이기 때문에 두 번째 그림에 있는 분포 형태가 유지되었을 것이다.
- Batch Normalization은 분포를 정규화함으로써 비선형 활성 함수의 의미를 살리는 개념이다.

# 04. Initialization
- 신경망이 처음 Weight를 랜덤하게 초기화 하고 Loss가 최소화되는 부분을 찾아가는데, 이때 랜덤하게 초기화하면 신경망의 초기 Loss가 달라지게 된다. 즉, 신경망을 어떻게 초기화하느냐에 따라 최적의 신경망 Loss(= global minimum)를 갖기 위해 걸리는 학습 속도가 달라질 수 있다.
### 1) LeCun Initialization
### 2) Xavier Initialization
- Sigmoid 활성함수와 함께 자주 쓰인다
### 3) He Initialization
- ReLU 활성함수와 함께 자주 쓰인다.

# 05. Optimizer
- Batch 단위로 Back Propagation하는 과정을 Stochastic Gradient Descent(SGD)라 하고 이러한 과정을 'Optimization'이라 한다.
- SGD 외에 SGD의 단점을 보완하기 위한 Optiizer들이 있다.
### 1) Momentum
- 미분을 통한 Gradient 방향으로가되, 일종의 관성을 추가한다.
- 이전의 업데이트 값을 다음 업데이트에 반영하는 것이라고 할 수 있다. 
### 2) Nesterov Accelerated Gradient(NAG)
- Momentum으로 이동한 후 Gradient를 구해 이동하는 방식이다.
### 3) Adaptive Gradient(Adagrad)
- '가보지 않은 곳은 많이 움직이고 가본 곳은 조금씩 움직이자'는 개념을 가지고 있음
- 즉, 많이 학습된 파라미터와 적게 학습된 파라미터에 learning rate를 다르게 하는 것으로 학습 정도에 따라 learning rate를 조정해 주는 것이다.
### 4) RMSProp
- Adagrad의 학습이 오래진행될수록 Step size가 작아진다는 단점을 보완한 방법이다.
- 최근에 학습이 많이 된 파라미터는 적게 학습하ㅏ고, 최근에 학습이 적게 된 파라미터는 많이 학습하는 것이다.
### 5) Adaptive Delta(Adadelta)
- Adagrad의 Gradient의 양이 너무 적어지면 움직임이 멈철 수 있다는 단점을 보완한 방법이다.
### 6) Adaptive Moment Estimation(Adam)
- 딥러닝 모델을 디자인할 때 가장 많이 사용하는 Optimizer이며, RMSProp과 Momentum 방식의 특징을 결합한 방법이다.
### 7) Rectified Adam optimizer(RAdam)
- 대부분의 optimizer는 가지고 있는 학습 초기에 Bad Local Optimum에 수렴해 버릴 수 있다는 단점이 있는데, 이는 학습 초기에 Gradient가 매우 작아져서 학습이 더 이상 일어나지 않는 현상을 말한다.이러한 Adaptive Learning Rate Term의 분산을 교정(Recify)하는 Optimizer이다.

# 06. 하이퍼파라미터 최적화
- 하이퍼파라미터의 '최적 값'이 존재하는 범위를 조금씩 줄여가는 것으로 우선 대략적인 범위를 설정하고, 무작위로 샘플링 한 뒤, 그 값으로 정확도를 평가하고, 이를 잘 살피면서 이 작업을 반복한다.
#### 0단계 : 하이퍼파라미터 값의 범위를 설정한다.
#### 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출한다.
#### 2단계 : 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정화고들 평가한다. (단, epoch은 작게 설정한다.)
#### 3단계 : 1단계와 2단계를 특정 횟수(100회 등) 반족하여, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.
